{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning - Deep Neural Networks",
      "provenance": [],
      "collapsed_sections": [
        "jhzKwoU6i65Y"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shIsmael/DeepLearning/blob/main/Deep_Learning_Deep_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnH9bVuI832D"
      },
      "source": [
        "Instructions:\n",
        "1. Make a copy of this notebook by opening the \"File\" tab and selecting \"Save a copy in Drive\"\n",
        "2. Close this tab and move to your copy of this notebook\n",
        "3. Follow the written guides within this notebook\n",
        "4. If instructed to, add your own code in the corresponding cell\n",
        "\n",
        "After completing this notebook, you will have:\n",
        "- An understanding of what deep neural networks are\n",
        "- An understanding of how to verify matrix/vector dimensions \n",
        "- Implemented cache to pass information from forwardpropagation to backpropagation\n",
        "- Implemented functions for forwardpropagation and backpropagation\n",
        "- An understanding of hyperparameters\n",
        "- Built a deep neural network to classify cats from non-cats!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0IHB9pBi6fC"
      },
      "source": [
        "### Deep Neural Networks and Notation\n",
        "In the context of deep learning, \"depth\" refers to the number of layers a neural network has (this is a *hyperparameter*, which we'll cover in this notebook). The logistic regression neural network (1 layer) and the shallow neural network (2 layers; remember, we don't count the input layer) that we built in the last notebook are both quite shallow. A neural network is essentially able to learn a nonlinear function based on its architecture and training data. By introducing more layers ($\\geq 3$), we can build networks that can model more complex functions to accomplish more complex tasks. We'll cover how to choose the appropriate number of layers for a given task later. Here is an example of a 3-layer neural network:\n",
        "\n",
        "![neuralnet](https://victorzhou.com/media/nn-series/network.png)\n",
        "\n",
        "Notation-wise:\n",
        "- We'll denote the number of layers in a network (again, not including the input layer) as $L$; Thus, $\\hat{y}$ is the activation of layer $L$\n",
        "\n",
        "- We'll use $n^{[l]}$ to denote the number of nodes in layer $l$. (In the above example, $n^{[2]} = 6$) \n",
        "- The input layer is $n^{[0]} = n_x$\n",
        "- The activations in layer $l$ will be denoted by $a^{[l]}$; Thus, $a^{[l]} = g^{[l]}(z^{[l]})$, where $g^{[l]}$ is the activation function for layer $l$\n",
        "- The weights and biases of layer $l$ are denoted by $W^{[l]}$ and $b^{[l]}$ (used to compute $z^{[l]}$)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEqVnxmgjL-r"
      },
      "source": [
        "# importing\n",
        "import time \n",
        "import numpy as np\n",
        "import h5py # used to retrieve the dataset, which is stored on an h5 file\n",
        "import matplotlib.pyplot as plt # for plotting graphs\n",
        "import scipy # used for post-training testing\n",
        "from PIL import Image # used for post-training testing\n",
        "from scipy import ndimage # used for post-training test\n",
        "\n",
        "# \"magic\" commands for configuring matplotlib plots that you don't have to worry about :)\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "np.random.seed(1) # setting a fixed seed is used for reproducibility\n",
        "\n",
        "# download the datasets from https://drive.google.com/drive/folders/1R5kzlNNvhABEm2oCpxwXsmmV48Vp1kQQ?usp=sharing and import them into this notebook when prompted\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn]))) \n",
        "\n",
        "# we'll be using the same cat vs non-cat dataset as seen in \"Logistic Regression with a Neural Network\"!\n",
        "def load_data():\n",
        "    train_dataset = h5py.File('train_catvnoncat.h5', \"r\") # we're opening this h5 file in read (\"r\") mode\n",
        "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
        "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
        "\n",
        "    test_dataset = h5py.File('test_catvnoncat.h5', \"r\")\n",
        "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
        "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
        "\n",
        "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
        "    \n",
        "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
        "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
        "    \n",
        "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n",
        "\n",
        "# loading our data\n",
        "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()\n",
        "\n",
        "# Explore your dataset \n",
        "m_train = train_x_orig.shape[0] # number of training examples\n",
        "num_px = train_x_orig.shape[1] # this should be 64, meaning that our images are 64x64x3\n",
        "m_test = test_x_orig.shape[0] # number of testing examples\n",
        "\n",
        "print(\"train_x_orig shape: \" + str(train_x_orig.shape))\n",
        "print(\"train_y shape: \" + str(train_y.shape))\n",
        "print(\"test_x_orig shape: \" + str(test_x_orig.shape))\n",
        "print(\"test_y shape: \" + str(test_y.shape))\n",
        "\n",
        "# reshaping and normalizing our examples \n",
        "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
        "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
        "\n",
        "# standardizing data to have feature values between 0 and 1.\n",
        "train_x = train_x_flatten/255.\n",
        "test_x = test_x_flatten/255.\n",
        "\n",
        "print(train_x.shape)\n",
        "print(test_x.shape)\n",
        "\n",
        "# with our dataset loaded and standardized, we can begin implementing the functions for forwardpropagation, backpropagation, and gradient descent!\n",
        "# these functions will be applicable to neural networks of any size, unlike the functions we defined in the previous notebooks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CN2sZw7njWEv"
      },
      "source": [
        "### Why Use Deep Neural Networks?\n",
        "For the sake of gaining intuition as to why we use deep neural networks, let's consider a type of neural network architecture that we'll cover in Chapter 6: a *convolutional neural network* (CNN). CNNs, originally created for image-related tasks, use *convolutional layers* as \"filters\" to extract image features. Therefore, starting from the first layer onwards, we can think of each layer as successively specializing in more complex tasks. For example, the first layer might recognize all of the edges in an image, the second might recognize corners, the third might recognize basic facial features (e.g. eyes, lips), the fourth might recognize portions of faces, and so on. This type of compositional representation applies to other types of data as well. For example, you might start by detecting basic waveform features in the case of auditory data. \n",
        "\n",
        "You can also think of deep neural networks as Taylor polynomials: adding more terms will result in higher accuracy and a better fit to the original (target) function, which may be extremely complex. As for choosing the number of hidden layers to use, you should try working up from a \"baseline\" number of hidden layers for a given task, though very deep architectures have been the best models for certain tasks in recent years. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM3VKq6wjCn6"
      },
      "source": [
        "### Forwardpropagation and Backpropagation\n",
        "Given a training example $x$ (or $a^{[0]}$), the general equations for forwardpropagation in a forward function are $z^{[l]} = W^{[l]}a^{[l-1]}+b^{[l]}$; $a^{[l]} = g^{[l]}(z^{[l]})$. For a training set $X$, the vectorized equations are $Z^{[l]} = W^{[l]}X + b^{[l]}$; $A^{[l]} = g^{[l]}(Z^{[l]})$ (To recap: we're stacking our training examples horizontally to perform forwardpropagation on the entire set). However, the above equations would be repeatedly computed in a for loop for each layer in the network, which is unavoidable. In addition, we'll be \"caching\" the linear computation $Z^{[l]}$ for backpropagation (computing the gradient of the cost function) when computing the activations of layer $l$. \n",
        "\n",
        "For backpropagation at layer $l$, (note that we'll be using different notation for derivatives) we can use these four equations for implementing our backward function:\n",
        "- $dZ^{[l]} = dA^{[l]} \\star g^{[l]\\prime}(Z^{[l]})$, where $g^{[l]\\prime}()$ denotes the derivative of $g^{[l]}$ the activation function of layer $l$; note that we're using our cached $Z^{[l]}$ from our forward function\n",
        "- $dW^{[l]} = dZ^{[l]} \\cdot A^{[l-1]T}$\n",
        "- $db^{[l]} = dZ^{[l]}$\n",
        "- $dA^{[l-1]} = W^{[l]T}\\cdot dZ^{[l]}$\n",
        "\n",
        "The forward and backward functions that you'll need to build your first deep neural network are implemented below. \n",
        "\n",
        "You will be using the cross-entropy cost function, defined as:\n",
        "$-\\frac{1}{m}\\sum^{m}_{i=1}(y^{(i)}log(a^{[L](o)})+(1-y^{(i)})log(1-a^{[L](i)}))$\n",
        "\n",
        "The update rule can be generalized as\n",
        "$W^{[l]} := W^{[l]} - \\alpha \\: dW^{[l]}$; $b^{[l]} := b^{[l]} - \\alpha \\: db^{[l]}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kpj34vPd7F-o"
      },
      "source": [
        "# firstly, let's define the activation functions and activation function derivative that we'll need to define our forward/backward function and our network\n",
        "\n",
        "# sigmoid! note that we are \"caching\" Z for later use in the computation of dZ\n",
        "def sigmoid(Z): # Z is the linear computation Wx+b    \n",
        "    A = 1/(1+np.exp(-Z))\n",
        "    cache = Z \n",
        "    return A, cache\n",
        "\n",
        "# backpropagation for a single sigmoid unit, where dA is the partial derivative of an activation, cache is the linear computation Z, and \n",
        "# dZ is the partial derivative of the cost with respect to Z\n",
        "def sigmoid_backward(dA, cache):    \n",
        "    Z = cache\n",
        "    s = 1/(1+np.exp(-Z)) # don't worry about this derivation\n",
        "    dZ = dA * s * (1-s)\n",
        "    assert (dZ.shape == Z.shape) # sanity check\n",
        "    return dZ\n",
        "\n",
        "# relu (rectified linear unit)! again, note that we're storing Z for later use\n",
        "def relu(Z):    \n",
        "    A = np.maximum(0,Z)   \n",
        "    assert(A.shape == Z.shape)\n",
        "    cache = Z \n",
        "    return A, cache\n",
        "\n",
        "# backpropagation for a single relu unit, where dA is the partial derivative of an activation, cache is the linear computation Z, and \n",
        "# dZ is the partial derivative of the cost with respect to Z\n",
        "def relu_backward(dA, cache):    \n",
        "    Z = cache\n",
        "    dZ = np.array(dA, copy=True) # don't worry about this derivation\n",
        "    dZ[Z <= 0] = 0\n",
        "    assert (dZ.shape == Z.shape)\n",
        "    return dZ\n",
        "\n",
        "# we can then implement a function to initialize our parameters for a given number of layers (quantified by L)\n",
        "def initialize_parameters_deep(layer_dims): # layer_dims will be a list containing the dimensions of each layer in our network    \n",
        "    np.random.seed(3)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)  # number of layers in the network\n",
        "\n",
        "    for l in range(1, L): # iterating over all layers\n",
        "        # parameters is a dictionary that stores the weights and biases of each layer\n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])*0.01 # weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1)) # bias vector of shape (layer_dims[l], 1)\n",
        "        # after this loop is finished, we can call, say, W1 and retrieve the weights for layer 1\n",
        "\n",
        "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1])) # sanity checks\n",
        "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "\n",
        "    return parameters\n",
        "\n",
        "# we can now implement our forwardpropagation module! \n",
        "\n",
        "#let's start by implementing a function that computes the vectorized version of Wx+b with caching:\n",
        "def linear_forward(A, W, b): # where A is the activation from the previous layer of shape (size of previous layer, number of examples)\n",
        "# use the previous notebook as reference for the shapes of the weights and biases    \n",
        "    Z = np.dot(W,A)+b # linear computation\n",
        "    assert(Z.shape == (W.shape[0], A.shape[1])) # sanity check\n",
        "    cache = (A, W, b) # cache (a tuple) will store these values for later use in backpropagation\n",
        "    \n",
        "    return Z, cache\n",
        "\n",
        "# we'll implement a function that applies the given activation function to linear_forward\n",
        "def linear_activation_forward(A_prev, W, b, activation): # where activation is a string specifying which activation function to use\n",
        "# and where A_prev is the activations from the previous layer in the same shape as specified in linear_forward\n",
        "    if activation == \"sigmoid\": # if the input to this function was \"sigmoid\", apply the sigmoid function and store the activation cache, as specified when we defined sigmoid\n",
        "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
        "        A, activation_cache = sigmoid(Z)\n",
        "    elif activation == \"relu\":\n",
        "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
        "        A, activation_cache = relu(Z)\n",
        "    \n",
        "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
        "    cache = (linear_cache, activation_cache) # a tuple containing linear_cache and activation_cache for computing backpropagation efficiently\n",
        "\n",
        "    return A, cache\n",
        "\n",
        "# finally, we can implement a function that performs vectorized forwardpropagation throughout the entire network\n",
        "# every hidden layer will use the ReLU activation function, and the output layer will use the sigmoid activation function, since we're doing binary classification\n",
        "def L_model_forward(X, parameters): # where X has the shape/dimensions (input size, number of examples) and parameters is the output of initialize_parameters_deep\n",
        "    caches = [] \n",
        "    A = X # redefining X as the activation of the input layer\n",
        "    L = len(parameters) // 2  # number of layers in the neural network\n",
        "    \n",
        "    for l in range(1, L): # iterating over layer every aside from the last layer, with \"l\" representing the current layer\n",
        "        A_prev = A # redefining the activation as the activation of the previous layer\n",
        "        A, cache = linear_activation_forward(A_prev,parameters['W'+str(l)],parameters['b' + str(l)],\"relu\") # using our function for the linear computation and activation\n",
        "        caches.append(cache) # adding to our cache for use in backpropagation\n",
        "    \n",
        "    # manually performing the linear -> activation for the final layer using the sigmoid activation function\n",
        "    AL, cache = linear_activation_forward(A,parameters['W'+str(L)],parameters['b'+str(L)],\"sigmoid\")\n",
        "    caches.append(cache) # adding to cache\n",
        "    \n",
        "    assert(AL.shape == (1,X.shape[1])) # sanity check\n",
        "            \n",
        "    return AL, caches # where AL is the last activation value (in other words, yhat)\n",
        "\n",
        "# we can now implement our cost function to check if our model is actually learning:\n",
        "def compute_cost(AL, Y): # where Y is a vector containing the true labels (0 if non-cat, 1 if cat) in the shape (1, num of examples)\n",
        "    m = Y.shape[1] # number of examples\n",
        "    # computing the cost, as defined in the equation above\n",
        "    cost = -(np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL)))/m  \n",
        "    cost = np.squeeze(cost) # making sure that the shape of cost is what we want\n",
        "    assert(cost.shape == ()) # sanity check\n",
        "    return cost\n",
        "\n",
        "# with our forwardpropagation functions and cost function defined, we can implement our backpropagation functions!\n",
        "\n",
        "# firstly, we'll implement a function to calculate dW, db, and dA_prev\n",
        "def linear_backward(dZ, cache): #dZ is defined by the equation above and cache is a tuple of the values (A_prev, W, b)\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1] # number of layers\n",
        "\n",
        "    # the equations for these computations are defined above\n",
        "    dW = (np.dot(dZ, A_prev.T))/m # we're dividing by m because we're using a vectorized implementation\n",
        "    db = (np.sum(dZ, axis = 1, keepdims = True))/m\n",
        "    dA_prev = np.dot(W.T, dZ)\n",
        "    \n",
        "    assert (dA_prev.shape == A_prev.shape) # sanity check\n",
        "    assert (dW.shape == W.shape)\n",
        "    assert (db.shape == b.shape)\n",
        "    \n",
        "    return dA_prev, dW, db\n",
        "\n",
        "# next, we'll define a function that merges linear_backward and the backward activation functions that we defined earlier\n",
        "def linear_activation_backward(dA, cache, activation): # where dA is the activation gradient of layer l and activation is a string specifying the desired activation\n",
        "    linear_cache, activation_cache = cache # note that cache is a tuple of values (linear_cache, activation_cache)\n",
        "    \n",
        "    if activation == \"relu\":\n",
        "        dZ = relu_backward(dA, activation_cache) # we're using the backward activation functions to obtain the partial deriative dZ for use in linear_backward\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "        \n",
        "    elif activation == \"sigmoid\":\n",
        "        dZ = sigmoid_backward(dA, activation_cache) \n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "    \n",
        "    return dA_prev, dW, db\n",
        "\n",
        "# finally, we can implement linear_activation_backward for the whole network\n",
        "# we'll use the cache from each layer from L_model_forward to backpropagate (compute derivatives/gradients) through each layer l\n",
        "def L_model_backward(AL, Y, caches): # caches is from L_model_forward\n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
        "    \n",
        "    # initializing the backpropagation by computing the partial derivative of AL (activation of last layer) with respect to the cost function\n",
        "    dAL = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # you don't need to know how this is derived\n",
        "    \n",
        "    # computing the last (Lth) layer's partial derivatives\n",
        "    current_cache = linear_activation_backward(dAL, caches[L-1], \"sigmoid\")\n",
        "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = current_cache\n",
        "    \n",
        "    # iterating over all other layers\n",
        "    for l in reversed(range(L-1)):\n",
        "        # lth layer partial derivatives\n",
        "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
        "        current_cache = linear_activation_backward(grads[\"dA\"+str(l+1)], caches[l], \"relu\") # using linear_activation_backward to compute the necessary partial derivatives\n",
        "        dA_prev_temp, dW_temp, db_temp = current_cache \n",
        "        grads[\"dA\" + str(l)] = dA_prev_temp # adding the partial derivatives to the gradient dictionary\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "    return grads # dictionary containing every partial derivative\n",
        "\n",
        "# finally, we can implement the update rule for all layers using the partial derivatives from grads\n",
        "def update_parameters(parameters, grads, learning_rate): # parameters and grads are both dictionaries    \n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "    # Update rule for each parameter. Use a for loop.\n",
        "    for l in range(L): # iterating over all layers\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads[\"dW\"+str(l+1)] # updating using the formula above\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads[\"db\"+str(l+1)]\n",
        "    return parameters # return updated parameters\n",
        "\n",
        "# you won't need manually implement the forward and backward functions once you start using a deep learning library (i.e. Tensorflow or PyTorch), but you're seeing \n",
        "# this manual implementation to gain some intuition as to what those libraries are actually doing behind the scenes :)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8K0yqCxjI3q"
      },
      "source": [
        "### Matrix Dimensions\n",
        "A common debugging strategy for verifying the validity of your inputs and your neural network architecture is manually verifying your matrix dimensions.\n",
        "\n",
        "For the architecture presented in the graphic in \"Deep Neural Networks and Notation\", the input dimensions are (4, 1) (more generally, the dimensions are ($n^{[0]}$, 1)), and the first layer's weight matrix dimensions are (6, 1). If we perform elementwise multiplication, our product should be a (6, 4) matrix. \n",
        "\n",
        "We can determine the shape of the weights of a layer $l$ by using the number of nodes in layer $l$ and the number of nodes in layer $l-1$: $W^{[l]} : (n^{[l]}, n^{[l-1]})$. Similarly, we can determine the shape of a given layer's bias vector with $b^{[l]} : (n^{[l]}, 1)$. During backpropagation, the shapes of dW and db should have the same shapes as $W^{[l]}$ and $b^{[l]}$. \n",
        "\n",
        "Using the properties given above, try working out the shapes in a vectorized implementation across $m$ training examples!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aQ-rWFxjhEB"
      },
      "source": [
        "### Hyperparameters\n",
        "The parameters in a neural network are the various weights and biases used in each layer of the network, which are iteratively updated to improve the network's performance via backpropagation and gradient descent. However, there are other parameters called *hyperparameters* that we need to set before training a neural network, an example of which is the *learning rate* $\\alpha$ (which you've seen in previous notebooks). \n",
        "\n",
        "The learning rate controls the magnitude of our updates; If we set $\\alpha$ too high, our updates are likely to overshoot the global minimum of the cost function, and if we set it too low, our updates will be too slow. Another hyperparameter is the number of iterations we should train for. If we train our network for too long, our network might become unable to make accurate predictions for examples outside of our training dataset through what is known as *overfitting*. Conversely, if we don't train long enough, our network might not be able to make accurate predictions on even the examples in the training dataset. \n",
        "\n",
        "More examples of a hyperparameters include the number of hidden layers and the number of nodes within those hidden layers. Generally, more hidden layers means that a network can model more complex functions, but has to train for a longer period of time to achieve reasonable accuracy. \n",
        "\n",
        "The choice of activation function for each layer, however, not a hyperparameter because it cannot be iteratively tuned. Since applied deep learning is an empirical process (i.e. the process is a cycle of idea -> code -> experiment), hyperparameters can be systemically tuned manually. However, you'll find that, as your neural network changes, the best choice of hyperparameters will change accordingly. Luckily, many deep learning libraries (i.e. Keras) have functions for automatically optimizing hyperparameters. \n",
        "\n",
        "Later in this course, we'll introduce more hyperparameters, such as momentum, minibatch size, and regularization hyperparameters. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cykMHwf1jPEz"
      },
      "source": [
        "### Putting It All Together\n",
        "Using the functions we've defined so far, define a function called model() that takes in train_x, train_y, layers_dims (a list containing the dimensions of each layer), the learning rate (a float), the number of iterations to train for (you can choose this), and a boolean variable *print_cost* that decides whether the function prints the cost every 100 examples. It should return a dictionary containing the final parameters of the model. After then, you should run the function by assigning it to a new variable with the required parameters. A template for this function is provided below (insert your code where indicated), along with the code for cost printing and a sample list for a 4-layer neural network. (Warning: Training may take a few minutes, so just be patient)\n",
        "\n",
        "Don't worry if you're unable to define the model() function; The fully defined function is provided under the next text cell, \"Check Your Work\". "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lluKYFyLkKAd"
      },
      "source": [
        "layers_dims = [12288, 20, 7, 5, 1] # sample list for 4-layer model, change as you wish as long as the first and layer layers remain constant\n",
        "\n",
        "\n",
        "def model(X, Y, layers_dims, learning_rate, num_iterations, print_cost):\n",
        "    costs = [] # keeping track of cost\n",
        "    \n",
        "    # INSERT CODE: initialize your parameters with layers_dims!\n",
        "    \n",
        "    # gradient descent loop!\n",
        "    for i in range(0, num_iterations): # iterating for the specified number\n",
        "        # INSERT CODE: forwardpropagation! you should be storing the returned values in new variables AL and caches\n",
        "        \n",
        "        # INSERT CODE: compute the cost! you should be storing the returned value in a new variable cost\n",
        "    \n",
        "        # INSERT CODE: perform backpropagation! you should be storing the returned value in a new variable grads\n",
        " \n",
        "        # INSERT CODE: update the parameters! you should be storing the returned value in a new variable parameters\n",
        "                \n",
        "        # print the cost every 100 training example if cost is true\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "        if print_cost and i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "\n",
        "    return parameters\n",
        "\n",
        "# default values for the function parameters if you don't know what to choose: learning_rate = 0.0075, num_iterations = 3000, print_cost = True\n",
        "\n",
        "# INSERT CODE: train the model by assigning it to a new variable named parameters!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QrvLeRGtRL3"
      },
      "source": [
        "# for fun: use your first deep neural network to classify an image of your choice!\n",
        "from google.colab import files # upload your image (in .jpg format) here\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn]))) \n",
        "\n",
        "my_image = \"insertfilename.jpg\" # change this string to the name of your image file\n",
        "my_label_y = [1] # insert the true class of your image (1 is cat, 0 is non-cat)\n",
        "\n",
        "# predicting!\n",
        "fname = \"images/\" + my_image\n",
        "image = np.array(ndimage.imread(fname, flatten=False))\n",
        "my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((num_px*num_px*3,1))\n",
        "my_image = my_image/255.\n",
        "my_predicted_image = predict(my_image, my_label_y, parameters)\n",
        "\n",
        "plt.imshow(image)\n",
        "print (\"y = \" + str(np.squeeze(my_predicted_image)) + \", your deep neural network predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhzKwoU6i65Y"
      },
      "source": [
        "### Check Your Work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hq7UD6Jo2rme"
      },
      "source": [
        "layers_dims = [12288, 20, 7, 5, 1]\n",
        "def model(X, Y, layers_dims, learning_rate, num_iterations, print_cost):\n",
        "    costs = []                     \n",
        "\n",
        "    parameters = initialize_parameters_deep(layers_dims)\n",
        "\n",
        "    for i in range(0, num_iterations):\n",
        "        AL, caches = L_model_forward(X, parameters)\n",
        "\n",
        "        cost = compute_cost(AL, Y)\n",
        "\n",
        "        grads = L_model_backward(AL, Y, caches)\n",
        "\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "        if print_cost and i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "    \n",
        "    return parameters\n",
        "\n",
        "parameters = model(train_x, train_y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost = True) # \"default\" parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKASXHIQjqW3"
      },
      "source": [
        "### Further Reading: Comparison to The Brain\n",
        "Similarly to a neural network's use of linear computation and activation, a biological neuron receives inputs from other neurons, does a simple thresholding computation, and if the resulting signal is within a certain range, the neuron fires a pulse of electricity outwards to other neurons. However, this comparison is extremely simple and outdated, as there is still much to be discovered as to how exactly neurons work and how the brain as a whole learns. We don't know if the brain updates its own neurons with something like gradient descent or if anything like forwardpropagation is used during computation, as the process of altering the state of neurons is still very mysterious. Furthermore, neurons can store memories depending on their connection to other neurons, whereas standard neural network rely on external data storage. "
      ]
    }
  ]
}