{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning - Shallow Neural Networks",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shIsmael/DeepLearning/blob/main/Deep_Learning_Shallow_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnH9bVuI832D"
      },
      "source": [
        "Instructions:\n",
        "1. Make a copy of this notebook by opening the \"File\" tab and selecting \"Save a copy in Drive\"\n",
        "2. Close this tab and move to your copy of this notebook\n",
        "3. Follow the written guides within this notebook\n",
        "4. If instructed to, add your own code in the corresponding cell\n",
        "\n",
        "After completing this notebook, you will have:\n",
        "- An understanding of hidden layers and units\n",
        "- An understanding of what activation functions are\n",
        "- An understanding of random initialization\n",
        "- An understanding of backpropagation for networks with hidden layers\n",
        "- Built a shallow neural network with one hidden layer to classify planar data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiWnrPfF1qMj"
      },
      "source": [
        "### Neural Networks and Vectorization\n",
        "Recall the \"neural network\" that we built and trained in the last notebook for binary classification, in which we used the equation $\\hat{y} = \\sigma(w^{T}x + b)$ for computing predictions. This equation, consisting of two calculations (the sigmoid is considered a separate calculation) can be used as a single *node* (or *unit*) within a neural network. Each node has a linear computation ($w^{T}x + b$) and an *activation function* (the purpose of which is to add some sort of nonlinear property to a node), which in this case is the sigmoid function. We can form a *layer* by \"stacking\" these nodes together; However, the nodes in a given layer will have no influence on each other. By stacking these layers, we can build a fully-fledged neural network! \n",
        "\n",
        "The first layer is referred to as the *input layer*, which, as you might guess, is where we can feed in input data. The last layer is referred to as the *output layer*, and any layers between the input and output layer are called *hidden layers*, since we don't know what goes on in the hidden layers. Each node is connected (in other words, it passes on its activation) to every node in the subsequent layer. Here's a simple neural network with two layers/one hidden layer (we'll be training a different architecture!): \n",
        "\n",
        "![shallow neural net](https://miro.medium.com/max/782/1*CfdaqnNb6RHLzPJTt1UXjQ.png)\n",
        "\n",
        "Notation-wise, we'll refer to the \"activation\" in the input as $a^{[0]} = X$.  We'll refer to the activations in the layer after the input layer as $a^{[1]}$ (don't confuse this with our notation for training examples!) which is derived from $a^{[1]} = \\sigma(W^{[1]T}x+b^{[1]})$, where, in the above image, $W^{[1]T}$ is a 4x3 matrix and $b^{[1]}$ is a 4x1 matrix, since there are $4*3 = 12$ connections from the input layer to the hidden layer and there are $4*1 = 4$ nodes in the hidden layer. The activation of the node at index $i$ (starting from the 1st node to the $n$th node) in layer $l$, each node's activation is referred to as $a^{[l]}_n$. \n",
        "\n",
        "For the purposes of demonstrating exactly how the input is propagated through the network, we'll use the first node in the hidden layer in the above image. It computes $a^{[1]}_1 = \\sigma(w^{[1]T}_1x + b^{[1]}_1)$, where $x$ is the three inputs $x_1\\dots x_3$. \n",
        "\n",
        "It turns out that computing every activation in layer $l$ with a for loop is quite inefficient, so we'll stack all of our weights $\\{w^{[1]}_1 \\dots w^{[1]}_n\\}$ vertically into a 4x3 matrix $W^{[1]}$ and stack our biases $\\{b^{[1]}_1 \\dots b^{[1]}_n\\}$ vertically into a 4x1 matrix $b^{[1]}$. We can then use vectorization (which you may recall from the Python tutorial) to perform the linear computations for the entire layer, resulting in $Z^{[1]}$. The activation function, sigmoid, can then be applied elementwise to $Z^{[1]}$ to calculate $a^{[1]}$. The activations $a^{[1]}$ can then be passed into the equation for the next layer, $a^{[2]} = \\sigma(W^{[2]T}a^{[1]}+b^{[2]})$ to repeat the process. \n",
        "\n",
        "Vectorization can similarly also be applied to the $m$ training examples $\\{x^{(1)}\\dots x^{(m)}\\}$ that we initially fed through the network with a for loop. Recall that $X$ is a matrix containing every training example as a column. We can then define the vectorizated implementation as follows for layer 1: $A^{[1]} = \\sigma(W^{[1]}X + b^{[1]})$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqBhfAu2qK2_"
      },
      "source": [
        "# importing the necessary libraries\n",
        "import numpy as np # you know what this is\n",
        "import matplotlib.pyplot as plt # for plotting graphs\n",
        "import h5py # for working with datasets stored on H5 files\n",
        "import sklearn # for generating our dataset\n",
        "import sklearn.datasets # for generating our dataset\n",
        "import sklearn.linear_model # for generating our dataset\n",
        "%matplotlib inline \n",
        "\n",
        "# loading our dataset, don't worry about the loading function\n",
        "def load_planar_dataset():\n",
        "    np.random.seed(1)\n",
        "    m = 400 # number of examples\n",
        "    N = int(m/2) # number of points per class\n",
        "    D = 2 # dimensionality\n",
        "    X = np.zeros((m,D)) # data matrix where each row is a single example\n",
        "    Y = np.zeros((m,1), dtype='uint8') # labels vector (0 for red, 1 for blue)\n",
        "    a = 4 # maximum ray of the flower\n",
        "\n",
        "    for j in range(2):\n",
        "        ix = range(N*j,N*(j+1))\n",
        "        t = np.linspace(j*3.12,(j+1)*3.12,N) + np.random.randn(N)*0.2 # theta\n",
        "        r = a*np.sin(4*t) + np.random.randn(N)*0.2 # radius\n",
        "        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
        "        Y[ix] = j\n",
        "        \n",
        "    X = X.T\n",
        "    Y = Y.T\n",
        "\n",
        "    return X, Y\n",
        "\n",
        "X, Y = load_planar_dataset() \n",
        "\n",
        "# visualizing the data: our goal is to build a model that defines these regions as either red or blue\n",
        "plt.scatter(X[0, :], X[1, :], c=Y, s=40, cmap=plt.cm.Spectral)\n",
        "\n",
        "# defining our neural network's architecture!\n",
        "# n_x is the size (number of nodes) of the input layer, n_h is the size of the hidden layer, and n_y is the size of the output layer\n",
        "def layer_sizes(X, Y):\n",
        "    # X: input dataset of shape (input size, number of examples)\n",
        "    # Y: labels of shape (output size, number of examples)\n",
        "  \n",
        "    n_x = X.shape[0] # we could hardcode this as 12288, but we'll use the shape of the input so any input can be used\n",
        "    n_h = 7 # we'll hardcode this as 7 for now, but you can play around with the number of hidden nodes in the final model\n",
        "    n_y = Y.shape[0]\n",
        "    return n_x, n_h, n_y\n",
        "\n",
        "# defining a random parameter initialization function\n",
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "    W1 = np.random.randn(n_h, n_x) # random.randn generates an array of random floats\n",
        "    b1 = np.zeros((n_h, 1))\n",
        "    W2 = np.random.randn(n_y, n_h)\n",
        "    b2 = np.zeros((n_y, 1))\n",
        "    \n",
        "    assert (W1.shape == (n_h, n_x)) # sanity checks\n",
        "    assert (b1.shape == (n_h, 1))\n",
        "    assert (W2.shape == (n_y, n_h))\n",
        "    assert (b2.shape == (n_y, 1))\n",
        "    \n",
        "    parameters = {\"W1\": W1, # weight matrix of shape (n_h, n_x)\n",
        "                  \"b1\": b1, # bias vector of shape (n_h, 1)\n",
        "                  \"W2\": W2, # weight matrix of shape (n_y, n_h)\n",
        "                  \"b2\": b2} # bias vector of shape (n_y, 1)\n",
        "    \n",
        "    return parameters\n",
        "\n",
        "# defining a forwardpropagation function to pass our inputs through our network according the architecture we've previously defined\n",
        "def forward_propagation(X, parameters): # parameters is the output of our random initialization function\n",
        "    # retrieving each parameter from the dictionary \"parameters\"\n",
        "    W1 = parameters[\"W1\"]\n",
        "    b1 = parameters[\"b1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    b2 = parameters[\"b2\"]\n",
        "    \n",
        "    # implementing forwardpropagation to calculate A2 (the prediction/activation of the output layer)\n",
        "    Z1 = np.dot(W1, X) + b1\n",
        "    A1 = np.tanh(Z1) \n",
        "    Z2 = np.dot(W2, A1) + b2\n",
        "    A2 = sigmoid(Z2)\n",
        "    \n",
        "    assert(A2.shape == (1, X.shape[1]))\n",
        "    \n",
        "    cache = {\"Z1\": Z1, # this will be used for optimization, since we need the values of each parameter to perform gradient descent!\n",
        "             \"A1\": A1,\n",
        "             \"Z2\": Z2,\n",
        "             \"A2\": A2}\n",
        "    \n",
        "    return A2, cache\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQCIRuxr7Hv-"
      },
      "source": [
        "### Activation Functions\n",
        "Up until now, we've just been using the sigmoid function as our activation function for all layers. However, there exist other activation functions that have their own uses, advantages, and drawbacks. \n",
        "\n",
        "Generally, the sigmoid function isn't the best function to be using for hidden layers, as the *hyperbolic tangent*/tanh function, defined as $\\frac{e^x - e^{-x}}{e^x + e^{-x}}$, almost always results in better performance. The tanh function is essentially a version of sigmoid that is bounded by -1 and 1 instead of 0 and 1. However, the sigmoid function is still useful in the cases where the output layer needs to output a real number between 0 and 1. \n",
        "\n",
        "A problem with both sigmoid and tanh is that, if the input is very large or small, then the gradient of either function will be very small, which can slow down gradient descent. The recently introduced *rectified linear unit*/ReLU function, defined by $max(0,x)$, solves this by fixing the derivative at either 0 (for 0 and negative inputs) and 1 (for positive inputs). Another variant of ReLU (that isn't used quite often) is the Leaky ReLU activation function, where the derivative for negative inputs is not 0 due to slightly sloping the graph downwards for negative inputs. \n",
        "\n",
        "The rule of thumb to remember is that ReLU/tanh is usually the best choice for hidden layers, while sigmoid should be used for binary classification problems. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_XK18wiqHQ2"
      },
      "source": [
        "# sigmoid\n",
        "def sigmoid(x):\n",
        "    s = 1/(1+np.exp(-x))\n",
        "    return s\n",
        "  \n",
        "# tanh can be called with np.tanh(x)!\n",
        "\n",
        "# ReLU!\n",
        "def ReLU(Z):  \n",
        "    A = np.maximum(0,Z)\n",
        "    return A"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxBMIscK7Pfo"
      },
      "source": [
        "### Backpropagation and Gradient Descent\n",
        "If we use the network defined in \"Neural Networks and Vectorization\", our parameters to train will be $W^{[1]}$, $b^{[1]}$, $W^{[2]}$, and $b^{[2]}$. We'll use the following cost function (cross-entropy loss): $J(W^{[1]}$, $b^{[1]}$, $W^{[2]}$, $b^{[2]}) = \\frac{1}{m}\\sum^{m}_{i=1} \\mathcal{L}(\\hat{y}, y)$; $\\mathcal{L}(\\hat{y}, y) = -(y^{(i)}log(a^{[2](i)}) + (1-y^{(i)})log(1-a^{[2](i)}))$. \n",
        "\n",
        "Our parameters will be initialized randomly (because we don't want every single hidden unit to calculate the same function!), and we'll predict for $m$ examples. Then, we can calculate the partial derivatives of the cost function $J$ and update each parameter accordingly (i.e. $W^{[1]} := W^{[1]} - \\alpha \\frac{\\partial J}{\\partial W^{[1]}}$). The gradient descent loop will repeat until our paramters look like they're converging towards the global minimum. \n",
        "Here are the equations for the partial derivatives of each parameter, which you'll implement yourself in the code cell below between the \"INSERT CODE HERE\" comments (note that the full partial derivative notation will be reduced to, say, $dW^{[1]}$ for the first weight matrix}:\n",
        "- $dW^{[1]} = \\frac{1}{m}dZ^{[1]}X^T$\n",
        "- $db^{[1]} = \\frac{1}{m}$ np.sum($dZ^{[1]}$, axis = 1, keepdims = True)\n",
        "- $dZ^{[1]} = W^{[2]T}dZ^{[2]}\\star g^{[1]\\prime} (Z^{[1]})$ (the apostrophe denotes the derivative of the first activation function)\n",
        "- $dW^{[2]} = \\frac{1}{m}dZ^{[2]}A^{[1]T}$\n",
        "- $db^{[2]} = \\frac{1}{m}$ np.sum($dZ^{[2]}$, axis = 1, keepdims = True)\n",
        "- $dZ^{[2]} = A^{[2]} - Y$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09vkVhBxqLby"
      },
      "source": [
        "def compute_cost(A2, Y, parameters):\n",
        "    \n",
        "    m = Y.shape[1] # number of example\n",
        "    logprobs = np.sum((Y*np.log(A2)+(1 - Y)*np.log(1 - A2)))\n",
        "    cost = -logprobs/m\n",
        "    \n",
        "    cost = float(np.squeeze(cost))  \n",
        "    assert(isinstance(cost, float))\n",
        "    \n",
        "    return cost\n",
        "\n",
        "# defining a function that performs backpropagation (in other words, calculates the partial derivatives of the cost function with respect to each parameter)\n",
        "def backward_propagation(parameters, cache, X, Y): # parameters is from initialize_parameters(), and cache is from forward_propagation()\n",
        "    m = X.shape[1] \n",
        "\n",
        "    # retrieving weights\n",
        "    W1 = parameters[\"W1\"] \n",
        "    W2 = parameters[\"W2\"]\n",
        "    \n",
        "    # retreiving cached activations\n",
        "    A1 = cache[\"A1\"]\n",
        "    A2 = cache[\"A2\"]\n",
        "    \n",
        "    # backpropagation!\n",
        "    # INSERT CODE HERE\n",
        "    dZ2 = None\n",
        "    dW2 = None\n",
        "    db2 = None\n",
        "    dZ1 = None\n",
        "    dW1 = None\n",
        "    db1 = None\n",
        "    # INSERT CODE HERE\n",
        "    \n",
        "    grads = {\"dW1\": dW1, # dictionary containing the partial derivatives with respect to each parameter\n",
        "             \"db1\": db1,\n",
        "             \"dW2\": dW2,\n",
        "             \"db2\": db2}\n",
        "    \n",
        "    return grads\n",
        "\n",
        "# next, we'll implement our update rules for gradient descent\n",
        "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
        "    # retrieving parameters\n",
        "    W1 = parameters[\"W1\"]\n",
        "    b1 = parameters[\"b1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    b2 = parameters[\"b2\"]\n",
        "\n",
        "    # retrieving gradients\n",
        "    dW1 = grads[\"dW1\"]\n",
        "    db1 = grads[\"db1\"]\n",
        "    dW2 = grads[\"dW2\"]\n",
        "    db2 = grads[\"db2\"]\n",
        "    \n",
        "    # updating each parameter\n",
        "    W1 = W1 - learning_rate*dW1\n",
        "    b1 = b1 - learning_rate*db1\n",
        "    W2 = W2 - learning_rate*dW2\n",
        "    b2 = b2 - learning_rate*db2\n",
        "    \n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    \n",
        "    return parameters # dictionary containing updated parameters "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4UF1G8R7aJO"
      },
      "source": [
        "### Putting It All Together\n",
        "Using the functions we've defined so far and the network defined in the previous notebook as reference, define a function called model() that takes in X_train, Y_train, n_h (the number of hidden nodes), a variable number of iterations to train for (you can choose this), and a boolean variable *print_cost* that decides whether the function prints the cost every 1000 iterations. It should return a dictionary containing the final parameters of the model. After then, you should run the function by assigning it to a new variable with the required parameters. A template for this function is provided below, along with the code for cost printing. (Warning: Training may take a few minutes, so just be patient)\n",
        "\n",
        "Don't worry if you're unable to define the model() function; The fully defined function is provided under the next text cell, \"Check Your Work\". "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QY1SQUqgqL9K"
      },
      "source": [
        "def nn_model(X, Y, n_h, num_iterations, print_cost):\n",
        "    n_x = layer_sizes(X, Y)[0]\n",
        "    n_y = layer_sizes(X, Y)[2]\n",
        "    \n",
        "    # INSERT CODE HERE: initialize your parameters here!\n",
        "\n",
        "    # gradient descent\n",
        "    for i in range(0, num_iterations):\n",
        "        # INSERT CODE HERE: perform forwardpropagation here! you should assign the return values to new variables \"A2\" and \"cache\"\n",
        "        \n",
        "        # INSERT CODE HERE: compute the cost here! you should assign the returned value to a new variable \"cost\"\n",
        " \n",
        "        # INSERT CODE HERE: compute the gradient of the cost function here! you should assign the returned value to a new variable \"grads\"\n",
        "\n",
        "        # INSERT CODE HERE: update the parameters here! you should assign the returned value to a new variable \"parameters\"\n",
        "        \n",
        "        # Print the cost every 1000 iterations\n",
        "        if print_cost and i % 1000 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "\n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLCORZWZWIlU"
      },
      "source": [
        "# INSERT CODE HERE: run your function here by assigning model() with all of the required parameters (pick 100 for n_h and any reasonable value around 10000 for num_iterations)\n",
        "\n",
        "# defining a prediction function to test your network!\n",
        "def predict(parameters, X):\n",
        "    A2, cache = forward_propagation(X, parameters)\n",
        "    predictions = (A2 > 0.5)\n",
        "    \n",
        "    return predictions\n",
        "\n",
        "# testing your network! you should get an accuracy of around 0.5, which is fairly good\n",
        "predictions = predict(parameters, X)\n",
        "print(\"predictions mean = \" + str(np.mean(predictions)))\n",
        "\n",
        "# plotting the decision boundary, you're not expected to understand the workings of this function\n",
        "def plot_decision_boundary(model, X, y):\n",
        "    # Set min and max values and give it some padding\n",
        "    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n",
        "    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n",
        "    h = 0.01\n",
        "    # Generate a grid of points with distance h between them\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "    # Predict the function value for the whole grid\n",
        "    Z = model(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    # Plot the contour and training examples\n",
        "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
        "    plt.ylabel('x2')\n",
        "    plt.xlabel('x1')\n",
        "    plt.scatter(X[0, :], X[1, :], c=y, cmap=plt.cm.Spectral)\n",
        "    \n",
        "plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\n",
        "plt.title(\"Decision Boundary for hidden layer size \" + str(4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLBZ5ND-TBGZ"
      },
      "source": [
        "### Check Your Work (Do not open unless finished)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOdv8pDXTDxz"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# backpropagation calculations!\n",
        "dZ2 = A2 - Y\n",
        "dW2 = (np.dot(dZ2, A1.T))/m\n",
        "db2 = (np.sum(dZ2, axis = 1, keepdims = True))/m\n",
        "dZ1 = W2.T*dZ2*(1 - np.power(A1, 2))\n",
        "dW1 = (np.dot(dZ1, X.T))/m\n",
        "db1 = (np.sum(dZ1, axis = 1, keepdims = True))/m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0FLPpggTja-"
      },
      "source": [
        "# the final model\n",
        "def model(X, Y, n_h, num_iterations, print_cost):\n",
        "    n_x = layer_sizes(X, Y)[0]\n",
        "    n_y = layer_sizes(X, Y)[2]\n",
        "    \n",
        "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
        "\n",
        "    for i in range(0, num_iterations):\n",
        "        A2, cache = forward_propagation(X, parameters)\n",
        "\n",
        "        cost = compute_cost(A2, Y, parameters)\n",
        " \n",
        "        grads = backward_propagation(parameters, cache, X, Y)\n",
        "\n",
        "        parameters = update_parameters(parameters, grads)\n",
        "        \n",
        "        if print_cost and i % 1000 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "\n",
        "    return parameters\n",
        "\n",
        "parameters = model(X, Y, n_h = 7, num_iterations = 10000, print_cost = True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}