{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4018fd7",
   "metadata": {},
   "source": [
    "# Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63a2600",
   "metadata": {},
   "source": [
    "### Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab081aef",
   "metadata": {},
   "source": [
    "$(x,y)$ where $x \\in R^{n_x}, y \\in {0,1}$\n",
    "$m$ training examples: $\\{x^{(1)},y^{(1)}, x^{(2)},y^{(2)},...,x^{(m)}, y^{(m)}\\}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fef60e",
   "metadata": {},
   "source": [
    "Is very common to denote the training examples as a matrix $X$ of type $n_x \\times m $ where $n_x$ is the number of features in the trainig example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ee5124",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afca3d5",
   "metadata": {},
   "source": [
    "### Sigmoid Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cf0cfd",
   "metadata": {},
   "source": [
    "$\\hat{y^{(i)}} = \\sigma(w^Tx^{(i)} + b)$, where $\\sigma(z^{(i)}) = \\dfrac{1}{1+e^{-z^{(i)}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1441cc4e",
   "metadata": {},
   "source": [
    "### Loss Funtion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c76f7d",
   "metadata": {},
   "source": [
    "$L(\\hat{y},y) = -(ylog\\hat{y} + (1-y)log(1-\\hat{y}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dabaed-1440-4581-8573-664fd0b749fc",
   "metadata": {},
   "source": [
    "We have to remember that the main purpose is to minimize the loss, so:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328960f9-9e13-4bb6-a7fe-a92d8494b40d",
   "metadata": {},
   "source": [
    "if $y=1$, $L(\\hat{y},y) = -log(\\hat{y})$, we want $log(\\hat{y})$ large, so we want \\hat{y} large, but $max(\\hat{y}) = 1$, which is exactly waht we want, if y=1 we want to $\\hat{y}=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883478a8-cfcd-4f03-8127-6cf8c19a9cc8",
   "metadata": {},
   "source": [
    "if $y = 0$, $L(\\hat{y},y) = -log(1-\\hat{y})$, due the negative signal, we want $log(1-\\hat{y})$ large, so $1-\\hat{y}$ large, as $\\hat{y}$ only can assume $0 or 1$, the max value will obtained when $\\hat{y} = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dc0dec-9950-452f-bbaa-19a8bee499a1",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ea3d83-8a01-41b6-8ea9-89117d82550c",
   "metadata": {},
   "source": [
    "For a simgle training example we can use the loss function, but for a set of training examples we use the Cost Function, who is a average of all Losses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b91a6a-306d-4c47-b58b-a0cdc791cc3c",
   "metadata": {},
   "source": [
    "$J(w,b) = -\\frac{1}{m} \\sum_{i=1}^{m} L(\\hat{y}^{(i)}, y^{(i)}) = $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fa27f8-a52d-4fab-8933-eefe7ecebedf",
   "metadata": {},
   "source": [
    "$J$ is a convex function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fc4797-d296-4803-8497-6b34ca13a20c",
   "metadata": {},
   "source": [
    "### Gradient Descent "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e339ae07-db37-490d-bd26-8a4ce36a4fe6",
   "metadata": {},
   "source": [
    "The algorithm of Gradien Descent is used to find the parameters of a function tha minimizes the function, in our case the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f919b82-2ad1-41da-baba-49bb59617662",
   "metadata": {},
   "source": [
    "Repeat { \n",
    "    $\\\\w : = w - \\alpha \\frac{\\partial J(w,b)}{\\partial w}\\\\$\n",
    "    $b : = b  - \\alpha \\frac{\\partial J(w,b)}{\\partial b}$\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bae40f1-1517-4c95-bfc3-5b274d4b1d7a",
   "metadata": {},
   "source": [
    "$\\alpha$ is the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c10e1d-9a86-4aaa-8074-b646c3f5ca39",
   "metadata": {},
   "source": [
    "![](images/lr_derivatives.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676a04bb-e166-4fbb-9310-836149d86271",
   "metadata": {},
   "source": [
    "In order to apply gradient descent in logistic regression, we'll need to compute two derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cbfee8-1047-4c55-8801-b811a8b94a58",
   "metadata": {},
   "source": [
    "Considering $z = wx + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f866385-0c97-43a1-95bb-12d54196fdbb",
   "metadata": {},
   "source": [
    "$\\dfrac{\\partial J(w,b)}{\\partial w} =  -\\dfrac{1}{m} \\sum_{i=1}^{m} \\dfrac{\\partial L(\\hat{y}^{(i)}, y^{(i)})}{\\partial w}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f89cc7-a70b-4360-8193-c465b09d6891",
   "metadata": {},
   "source": [
    "$\\dfrac{\\partial L(\\hat{y}^{(i)}, y^{(i)})}{\\partial w} = \\dfrac{\\partial L(\\hat{y}^{(i)}, y^{(i)})}{\\partial \\hat{y}^{(i)}} \\times \\dfrac{\\partial \\hat{y}^{(i)}}{\\partial z} \\times \\dfrac{\\partial z}{\\partial w}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88e39ba-5978-429e-b24e-fc2fa8cce5df",
   "metadata": {},
   "source": [
    "$\\dfrac{\\partial L(\\hat{y}^{(i)}, y^{(i)})}{\\partial \\hat{y}^{(i)}} = -\\dfrac{y}{\\hat{y}^{(i)}} + \\dfrac{1-y}{1-\\hat{y}^{(i)}} = \\dfrac{-(1-\\hat{y}^{(i)})\\cdot y + (1-y)\\hat{y}^{(i)}}{\\hat{y}^{(i)} \\cdot (1-\\hat{y}^{(i)}) } = \\dfrac{-y +\\hat{y}^{(i)}y +\\hat{y}^{(i)} -\\hat{y}^{(i)}y}{\\hat{y}^{(i)} \\cdot (1-\\hat{y}^{(i)}) } = \\dfrac{\\hat{y}^{(i)} -y}{\\hat{y}^{(i)} \\cdot (1-\\hat{y}^{(i)}) }$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cbae54-00d4-4bf9-8aa5-6cad9fb2b1c2",
   "metadata": {},
   "source": [
    "$\\dfrac{\\partial \\hat{y}^{(i)}}{\\partial z} =\\dfrac{e^{-(wx + b)}}{(1+e^{-(wx + b)})^2} = \\dfrac{1}{1+e^{-(wx + b)}} \\cdot \\dfrac{e^{-(wx + b)}}{1+e^{-(wx + b)}} = \\dfrac{1}{1+e^{-(wx + b)}} \\Big(\\cdot 1 - \\dfrac{1}{1+e^{-(wx + b)}}\\Big) = \\hat{y}^{(i)}(1 - \\hat{y}^{(i)}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087dacdb-fecc-407d-a322-d1552c0f4c82",
   "metadata": {},
   "source": [
    "$\\dfrac{\\partial z}{\\partial w} = x$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6897d7bf-29a8-4f9b-abc8-21dc570777b6",
   "metadata": {},
   "source": [
    "$\\dfrac{\\partial L(\\hat{y}^{(i)}, y^{(i)})}{\\partial w} = \\dfrac{\\hat{y}^{(i)} -y}{\\hat{y}^{(i)} \\cdot (1-\\hat{y}^{(i)}) } \\times \\hat{y}^{(i)}(1 - \\hat{y}^{(i)}) \\times x = x \\times (\\hat{y}^{(i)} -y) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88305e9a-f24c-4f45-a216-95cf6edec7a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
